{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "W&B-RayTune.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmQ6JHH3KSAJ"
      },
      "source": [
        "<img src=\"https://i.imgur.com/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\" />\n",
        "\n",
        "# Quickstart\n",
        "Use Weights & Biases for machine learning experiment tracking, dataset versioning, and project collaboration.\n",
        "\n",
        "<div><img /></div>\n",
        "\n",
        "<img src=\"https://i.imgur.com/uEtWSEb.png\" width=\"650\" alt=\"Weights & Biases\" />\n",
        "\n",
        "<div><img /></div>\n",
        "\n",
        "#Weights & Biases ðŸ’œ Ray Tune\n",
        "Both Weights and Biases and Ray/Tune are built for scale and handle millions of models every month for teams doing some of the most cutting-edge deep learning research.\n",
        "\n",
        "Whereas W&B is a centralized repository for everything you need to track, reproduce and gain insights from your models easily; Ray/Tune provides a simple interface for scaling and running distributed experiments. A few reasons why our community likes Ray/Tune â€“\n",
        "\n",
        "* Simple Distributed execution: Ray Tune makes it easy to scale from a single node, to multiple GPUs, and further multiple nodes.\n",
        "* Large number of algorithms: Ray Tune has a huge number of algorithms including Population Based Training, ASHA, and HyperBand\n",
        "* Framework agnostic: Ray Tune works across frameworks including PyTorch, Keras, Tensorflow, XGBoost, and PyTorchLightning.\n",
        "* Fault-tolerance: Ray Tune is built on top of Ray, providing fault tolerance out of the box.\n",
        "\n",
        "Let's start by inastalling the libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EegpOpMiNf0"
      },
      "source": [
        "!pip install -qq filelock==3.0.12\n",
        "!pip install -U -qq ray==0.8.7\n",
        "!pip install -qq wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "topVy2BhZRbd"
      },
      "source": [
        "# Reproducibility\n",
        "In order to make this experiment reproducible, we'll set the seeds for random number generators of various libraries used in this experiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKmyNQh5iXnV"
      },
      "source": [
        "import getpass\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
        "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
        "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
        "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2AldspdQLlQ"
      },
      "source": [
        "# W&B Ray Tune Integration\n",
        "**W&B** integrates with `Ray` by offering two lightweight standalone integrations:\n",
        "\n",
        "1.   You can either use `WandbLogger`, which automatically logs metrics reported to Tune to the Wandb API along with the configurations of the experiment. Here is an example usage.\n",
        "```python\n",
        " def train(config):\n",
        "        ...\n",
        "        tune.report(accuracy=acc) #This reported metric will be logged by WandbLogger to W&B dashboard\n",
        "...\n",
        "analysis = tune.run(train ,\n",
        "                  loggers = [WandbLogger],\n",
        "                  ...\n",
        "                    )\n",
        "\n",
        "2. You might run into scenarios where you'd like to log custom metrics, plots and outputs(including media files) directly to your W&B Dashboard. This can be accomplised by using `@wandb_mixin`.\n",
        "\n",
        " `@wandb_mixin` is a function decorator, which can be used with the function API. It automatically initializes the Wandb API with Tuneâ€™s training information. This allows you to use `wandb.log()` within the function scope that uses this decorator.\n",
        "You can just use the Wandb API like you would normally do, e.g. using `wandb.log()` to log your training process, charts and media files.\n",
        "Example usage:\n",
        "```python\n",
        "@wandb_mixin\n",
        "def train(config):\n",
        "      ...\n",
        "      wandb.log({\n",
        "                  \"Accuracy\" : acc,\n",
        "                \"Output\" : wandb.Image(output_grid),\n",
        "                })\n",
        "...\n",
        "tune.run(train,\n",
        "          ...  \n",
        "             )\n",
        "```\n",
        "\n",
        "**Both of these methods can be used together or independently.**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GY7z2xGnk5XA"
      },
      "source": [
        "from ray import tune\n",
        "from ray.tune.examples.mnist_pytorch import ConvNet, get_data_loaders, test, train\n",
        "from ray.tune.integration.wandb import wandb_mixin\n",
        "import torch.optim as optim\n",
        "import wandb\n",
        "\n",
        "@wandb_mixin\n",
        "def train_mnist(config):\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    train_loader, test_loader = get_data_loaders()\n",
        "\n",
        "    model = ConvNet()\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"])\n",
        "    \n",
        "    for i in range(10):\n",
        "        train(model, optimizer, train_loader, device=device)\n",
        "        acc = test(model, test_loader, device=device)\n",
        "\n",
        "        # When using WandbLogger, the metrics reported to tune are also logged in the W&B dashboard\n",
        "        tune.report(mean_accuracy=acc)\n",
        "\n",
        "        # @wandb_mixin enables logging custom metric using wandb.log()\n",
        "        error_rate = 100*(1-acc)\n",
        "        wandb.log({ \"Error Rate \" : error_rate})\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jCZhkqQoVlk"
      },
      "source": [
        "Wandb configuration is done by passing a wandb key to the config parameter of `tune.run()` (see detailed example below).\n",
        "\n",
        "The content of the `wandb` config entry is passed to `wandb.init()` as keyword arguments. The exception are the following settings, which are used to configure the `wandb` itself:\n",
        "```\n",
        "Parameters\n",
        "api_key_file (str) â€“ Path to file containing the Wandb API KEY.\n",
        "\n",
        "api_key (str) â€“ Wandb API Key. Alternative to setting api_key_file.\n",
        "```\n",
        "\n",
        "Wandbâ€™s `group`, `run_id` and `run_name` are automatically selected by Tune, but can be overwritten by filling out the respective configuration values.\n",
        "\n",
        "Please see here for all other valid configuration settings: https://docs.wandb.com/library/init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXBnHdK9lbBy"
      },
      "source": [
        "from ray.tune.integration.wandb import WandbLogger\n",
        "\n",
        "api_key = getpass.getpass(\"Enter your W&B apikey from https://wandb.ai/settings : \")\n",
        "\n",
        "analysis = tune.run(\n",
        "    train_mnist,\n",
        "    loggers=[WandbLogger], # WandbLogger logs experiment configurations and metrics reported via tune.report() to W&B Dashboard \n",
        "\n",
        "     resources_per_trial={'gpu': 1},\n",
        "     config={\n",
        "        #wandb dict accepts all arguments that can be passed in wandb.init() \n",
        "        \"wandb\": {\"project\": \"raytune\", \"job_type\":'raytune-demo' , 'api_key': api_key},\n",
        "\n",
        "        # Hyperparameters\n",
        "        \"lr\": tune.grid_search([0.0001, 0.001, 0.1]),\n",
        "        \"momentum\": tune.grid_search([0.9, 0.99])    \n",
        "    })\n",
        "\n",
        "print(\"Best config: \", analysis.get_best_config(metric=\"mean_accuracy\"))\n",
        "\n",
        "# Get a dataframe for analyzing trial results.\n",
        "df = analysis.dataframe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwZrL7Qh4OLS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}